Integrantes do grupo: Thainan Remboski, Pablo Kila, Daniel Farias

Tarefa A: Implementação em OpenMP


#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <limits.h>

#define NUM_THREADS 4 	// Define o numero de threads

int main(int argc, char *argv[])
{
	int i = 0;
	int circle_points = 0; // Quantidade de pontos que se encontram dentro do circulo
	int total_points = 100000; // Total de pontos calculados, valor usado nos testes iniciais
	int num_threads = NUM_THREADS;	// Seta a variavel num_threads com o numero de threads definido
	double x = 0.0, y = 0.0; // Pontos a serem determinados pelo metodo usado
	double pi = 0.0; // Variavel que recebe o resultado final

	omp_set_num_threads(num_threads);	// Seta o numero de threads a ser utilizado


// Os valores de x e y não devem se compartilhadas por isso são passados como private, entretanto o valor de 
//circle_points deve ser recebido 
//como a soma de todos os valores encontrados para essa variavel, portanto é usado reduction
	#pragma omp parallel private(x, y) reduction(+:circle_points)
	{
		#pragma omp for
		for (i = 0; i < total_points; i++) {
			x = drand48(); // Funcao usada para calcular valores entre 0 e 1
			y = drand48(); //com ela a velocidade de execução aumenta, entretanto a precisão acaba sendo reduzida
			if (x * x + y * y <= 1) {
				circle_points++; // Se o valor gerado se encontra no circulo, adiciona esse número na variavel usada no calculo
			}
		}
	}
	pi = (double)4 * (double)circle_points / (double)total_points; // Funcao usada na especificacao do trabalho
	printf("Pi = %f\n", pi);
	return 0;
}


----------------------------------------------------------------------------------------------------------------------

Tarefa B: Implementação usando MPI


#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <limits.h>
#include "omp.h"

#define NUM_THREADS 8
#define NUM_ITER INT_MAX

int monte_carlo_partition(int end_value) {

	int i = 0;
	int circle_points = 0; // Quantidade de pontos que se encontram dentro do circulo
	double x = 0.0, y = 0.0; // Pontos a serem determinados pelo metodo usado
	double z = 0.0; // Valor que define se um ponto se encontra dentro de um circulo

	srandom((int)time(NULL)); // Gerador de numeros pseudo aleatorios

// num_threads se refere a quantidade de threads usadas nos testes iniciais 
	#pragma omp parallel private(x, y) reduction(+:circle_points) num_threads(NUM_THREADS)
	{
		#pragma omp for
		for (i = 0; i < end_value; i++) {
			x = (double)random() / (double)RAND_MAX; // Funcao usada para calcular os pontos, RAND_MAX sera sempre maior ou igual a 
			y = (double)random() / (double)RAND_MAX; // Random, fazendo o resultado variar de 0 a 1
			z = sqrt((x * x) + (y * y)); // Calculo da localizacao do ponto
			if (z <= 1.0) { circle_points++; } // Determina se faz parte do circulo
		}
	}
	return circle_points;
}

int main(int argc, char** argv) {

	MPI_Status status; // Recebe quem enviou a mensagem e seu tag
	int i = 0, proc = 0;
	int circle_points = 0; // Valor total usado no calculo final
	int circle_points_part = 0; // Valor recebido de cada interação com MPI 
	int num_iter = NUM_ITER; // Quantidade de iteracoes usadas no teste
	int num_proc = 0, proc_id = 0; // Quantidade e ids dos processos que serao usados
	int master = 0, tag = 1313; // Variaveis necessarias para o MPI
	double pi = 0.0; // Variavel que recebe o resultado final

// Inicializacao do MPI
	MPI_Init(&argc, &argv); 
	MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
	MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);

// Particoes serao o total de iteracoes dividido pelo numero de processos menos um que recebe o resultado final
	int part = num_iter / (num_proc - 1);

// Dependendo da id do processo sera feito Send ou Recv
	if (proc_id == 0) { // Processo usado para receber o resultado da execucao

		for (proc = 1; proc < num_proc; proc++) {
			MPI_Recv(&circle_points_part, 1, MPI_REAL, proc, tag, MPI_COMM_WORLD, &status);
			circle_points += circle_points_part;
	}
	pi = (double)circle_points / (double)num_iter * (double)4.0; // Funcao usada na especificacao do trabalho
	printf("Pi = %f\n", pi);

	} else {
		circle_points_part = monte_carlo_partition(part);
		MPI_Send(&circle_points_part, 1, MPI_REAL, master, tag, MPI_COMM_WORLD);
	}
	MPI_Finalize(); // Termina o MPI
}



------------------------------------------------------------------------------------------------------------------------------------

Tarefa C: Descrição dos recursos de paralelismo e Análise de desempenho da implementação da tarefa A

Descrição dos recursos de paralelismo:

	## Tarefa A ##
	A ferramenta de programação multithread utilizada na tarefa A foi OpenMP.

	A quantidade de threads foi setada no parâmetro da função omp_set_num_threads.
	A região paralela foi criada com a diretiva "#pragma omp parallel", onde foram passadas duas variáveis privadas (x e y) e uma
	variável de redução para soma (circle_points). Essa variável é utilizada para armazenar o número de pontos no interior do círculo.
	No interior da região paralela, for utilizada a diretiva "#pragma omp for" para gerar as threads que vão iterar sobre o total de
	pontos, gerando os pontos que pertecerão ou não ao interior do círculo.
	No final, a variável "circle_points" vai estar com o total de pontos gerados no interior do círculo.

	## Tarefa B ##
	As ferramentas de programação multithread utilizadas na tarefa B forão OpenMP e MPI.

	Na função "monte_carlo_partition":
	A região paralela foi criada com a diretiva "#pragma omp parallel", onde foram passadas duas variáveis privadas (x e y) e uma
	variável de redução para soma (circle_points). Essa variável é utilizada para armazenar o número de pontos no interior do círculo.
	Também foi setado o número de threads a ser utilizado.
	No interior da região paralela, for utilizada a diretiva "#pragma omp for" para gerar as threads que vão iterar sobre o total de
	pontos, gerando os pontos que pertecerão ou não ao interior do círculo.
	No final, a variável "circle_points" vai estar com o total de pontos gerados no interior do círculo.

	No "main":
	O MPI é inicializado com a função "MPI_Init" e logo depois são setados o "size" e o "rank" com as funções "MPI_Comm_size" e
	"MPI_Comm_rank".
	Todos os processos com exceção do processo zero (master) são responsáveis por chamar a função "monte_carlo_partition" para
	coletar os resultados parciais e mandá-los para o "master" através da função "MPI_Send".
	O processo "master" recebe todos os resultados parciais através da função "MPI_Recv". O resultado final é gerado e mostrado.
	Ao término, o MPI é finalizado pela função "MPI_Finalize".


Desempenho:
	Abaixo encontra-se uma tabela com o tempo de execução da tarefa A, foram usadas 0, 1, 2, 4, 8, 16, 32 e 64 threads, sendo 0 threads, a implementação serial. Os valores 10000, 100000, 1000000 e 10000000 se referem a quantidade de pontos gerados pelo montecarlo.
	Os valores da tabela referem-se a média do tempo de execução de 10 execuções do algoritmo. Esses valores foram adquridos através da função "time" do Linux.

		10000			100000			1000000			10000000
	0	0.0014s			0.0033s			0.0217s			0.2318s
	1	0.0012s			0.0041s			0.028s			0.269s
	2	0.0014s			0.004s			0.0291s			0.2722s
	4	0.0015s			0.004s			0.0295s			0.2966s
	8	0.0019s			0.004s			0.0283s			0.2682s
	16	0.002s			0.004s			0.0289s			0.2701s
	32	0.002s			0.005s			0.0299s			0.2702s
	64	0.004s			0.0061s			0.0307s			0.2702s


	De acordo com os resultados obtidos pela média de 10 execuções, nota-se que o aumento da quantidade de threads não apresenta
	uma mudança significativa no tempo de execução do algoritmo de Monte Carlo.
	Um dos fatores mais significativos analisados, é a diferença entre a execução do algoritmo de forma paralela e serial.
	Na execução serial, o algoritmo se mostrou levemente mais rápido do que na execução paralela com 1 ou 2 threads. Isso pode ser
	atribuído ao overhead de criação das threads, porém esse fator não se mostra tão presente nas diferentes execuções paralelas.
	Como já era esperado, a diferença mais visível no desempenho se deu na mudança do total de pontos calculados, onde quanto maior
	o número de pontos, maior o tempo de execução do algoritmo.
	Isso se deve, provavelmente, pelo algoritmo não ser muito complexo, onde a utilização de threads não aparenta ser muito necessária.