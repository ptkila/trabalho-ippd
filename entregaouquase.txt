Integrantes do grupo: Thainan Remboski, Pablo Kila, Daniel Farias

Tarefa A: Implementação em OpenMP


#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <limits.h>

#define NUM_THREADS 4 	// Define o numero de threads

int main(int argc, char *argv[])
{
	int i = 0;
	int circle_points = 0; // Quantidade de pontos que se encontram dentro do circulo
	int total_points = 100000; // Total de pontos calculados, valor usado nos testes iniciais
	int num_threads = NUM_THREADS;	// Seta a variavel num_threads com o numero de threads definido
	double x = 0.0, y = 0.0; // Pontos a serem determinados pelo metodo usado
	double pi = 0.0; // Variavel que recebe o resultado final

	omp_set_num_threads(num_threads);	// Seta o numero de threads a ser utilizado


// Os valores de x e y não devem se compartilhadas por isso são passados como private, entretanto o valor de 
//circle_points deve ser recebido 
//como a soma de todos os valores encontrados para essa variavel, portanto é usado reduction
	#pragma omp parallel private(x, y) reduction(+:circle_points)
	{
		#pragma omp for
		for (i = 0; i < total_points; i++) {
			x = drand48(); // Funcao usada para calcular valores entre 0 e 1
			y = drand48(); //com ela a velocidade de execução aumenta, entretanto a precisão acaba sendo reduzida
			if (x * x + y * y <= 1) {
				circle_points++; // Se o valor gerado se encontra no circulo, adiciona esse número na variavel usada no calculo
			}
		}
	}
	pi = (double)4 * (double)circle_points / (double)total_points; // Funcao usada na especificacao do trabalho
	printf("Pi = %f\n", pi);
	return 0;
}


----------------------------------------------------------------------------------------------------------------------

Tarefa B: Implementação usando MPI


#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <limits.h>
#include "omp.h"

#define NUM_THREADS 8
#define NUM_ITER INT_MAX

int monte_carlo_partition(int end_value) {

	int i = 0;
	int circle_points = 0; // Quantidade de pontos que se encontram dentro do circulo
	double x = 0.0, y = 0.0; // Pontos a serem determinados pelo metodo usado
	double z = 0.0; // Valor que define se um ponto se encontra dentro de um circulo

	srandom((int)time(NULL)); // Gerador de numeros pseudo aleatorios

// num_threads se refere a quantidade de threads usadas nos testes iniciais 
	#pragma omp parallel private(x, y) reduction(+:circle_points) num_threads(NUM_THREADS)
	{
		#pragma omp for
		for (i = 0; i < end_value; i++) {
			x = (double)random() / (double)RAND_MAX; // Funcao usada para calcular os pontos, RAND_MAX sera sempre maior ou igual a 
			y = (double)random() / (double)RAND_MAX; // Random, fazendo o resultado variar de 0 a 1
			z = sqrt((x * x) + (y * y)); // Calculo da localizacao do ponto
			if (z <= 1.0) { circle_points++; } // Determina se faz parte do circulo
		}
	}
	return circle_points;
}

int main(int argc, char** argv) {

	MPI_Status status; // Recebe quem enviou a mensagem e seu tag
	int i = 0, proc = 0;
	int circle_points = 0; // Valor total usado no calculo final
	int circle_points_part = 0; // Valor recebido de cada interação com MPI 
	int num_iter = NUM_ITER; // Quantidade de iteracoes usadas no teste
	int num_proc = 0, proc_id = 0; // Quantidade e ids dos processos que serao usados
	int master = 0, tag = 1313; // Variaveis necessarias para o MPI
	double pi = 0.0; // Variavel que recebe o resultado final

// Inicializacao do MPI
	MPI_Init(&argc, &argv); 
	MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
	MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);

// Particoes serao o total de iteracoes dividido pelo numero de processos menos um que recebe o resultado final
	int part = num_iter / (num_proc - 1);

// Dependendo da id do processo sera feito Send ou Recv
	if (proc_id == 0) { // Processo usado para receber o resultado da execucao

		for (proc = 1; proc < num_proc; proc++) {
			MPI_Recv(&circle_points_part, 1, MPI_REAL, proc, tag, MPI_COMM_WORLD, &status);
			circle_points += circle_points_part;
	}
	pi = (double)circle_points / (double)num_iter * (double)4.0; // Funcao usada na especificacao do trabalho
	printf("Pi = %f\n", pi);

	} else {
		circle_points_part = monte_carlo_partition(part);
		MPI_Send(&circle_points_part, 1, MPI_REAL, master, tag, MPI_COMM_WORLD);
	}
	MPI_Finalize(); // Termina o MPI
}



------------------------------------------------------------------------------------------------------------------------------------

Tarefa C: Descrição dos recursos de paralelismo e Análise de desempenho da implementação da tarefa A

Descrição dos recursos de paralelismo:

	## Tarefa A ##
	A ferramenta de programação multithread utilizada na tarefa A foi OpenMP.

	A quantidade de threads foi setada no parâmetro da função omp_set_num_threads.
	A região paralela foi criada com a diretiva "#pragma omp parallel", onde foram passadas duas variáveis privadas (x e y) e uma
	variável de redução para soma (circle_points). Essa variável é utilizada para armazenar o número de pontos no interior do círculo.
	No interior da região paralela, for utilizada a diretiva "#pragma omp for" para gerar as threads que vão iterar sobre o total de
	pontos, gerando os pontos que pertecerão ou não ao interior do círculo.
	No final, a variável "circle_points" vai estar com o total de pontos gerados no interior do círculo.

	## Tarefa B ##
	As ferramentas de programação multithread utilizadas na tarefa B forão OpenMP e MPI.

	Na função "monte_carlo_partition":
	A região paralela foi criada com a diretiva "#pragma omp parallel", onde foram passadas duas variáveis privadas (x e y) e uma
	variável de redução para soma (circle_points). Essa variável é utilizada para armazenar o número de pontos no interior do círculo.
	Também foi setado o número de threads a ser utilizado.
	No interior da região paralela, for utilizada a diretiva "#pragma omp for" para gerar as threads que vão iterar sobre o total de
	pontos, gerando os pontos que pertecerão ou não ao interior do círculo.
	No final, a variável "circle_points" vai estar com o total de pontos gerados no interior do círculo.

	No "main":
	O MPI é inicializado com a função "MPI_Init" e logo depois são setados o "size" e o "rank" com as funções "MPI_Comm_size" e
	"MPI_Comm_rank".
	Todos os processos com exceção do processo zero (master) são responsáveis por chamar a função "monte_carlo_partition" para
	coletar os resultados parciais e mandá-los para o "master" através da função "MPI_Send".
	O processo "master" recebe todos os resultados parciais através da função "MPI_Recv". O resultado final é gerado e mostrado.
	Ao término, o MPI é finalizado pela função "MPI_Finalize".


Desempenho:
	Abaixo encontra-se uma tabela com o tempo de execução da tarefa A, foram usadas 0, 1, 2, 4, 8, 16, 32 e 64 threads, sendo 0 threads, a implementação serial. Os valores 10000, 100000, 1000000 e 10000000 se referem a quantidade de pontos gerados pelo montecarlo.
	Os valores da tabela referem-se a média do tempo de execução de 10 execuções do algoritmo. Esses valores foram adquridos através da função "time" do Linux.

	10000			100000			1000000			10000000
0	0.0014s			0.0033s			0.0217s			0.2318s
1	0.0012s			0.0041s			0.028s			0.269s
2	0.0014s			0.004s			0.0291s			0.2722s
4	0.0015s			0.004s			0.0295s			0.2966s
8	0.0019s			0.004s			0.0283s			0.2682s
16	0.002s			0.004s			0.0289s			0.2701s
32	0.002s			0.005s			0.0299s			0.2702s
64	0.004s			0.0061s			0.0307s			0.2702s

	A quantidade de threads foi setada no parâmetro da função omp_set_num_threads.
	De acordo com a execução nota-se que o tempo de execução acaba tendo pouca variação com menos pontos sendo gerados, 
	entretanto, com 64 threads o tempo acaba sendo pior que as outras quantidades, pelo menos até o valor de 10000000 pontos, 
	isso ocorre pelo tempo necessário para criar e sincronizar um número maior de threads mesmo que o problema seja simples,
	criar um valor aleatório e realizar um calculo são funções que não exigem muito processamento, entretanto conforme o 
	número de pontos aumenta é possível notar uma aproximação entre os tempos de uso de 64 e 32 threads, isso indica um 
	melhor desempenho no uso de threads a partir de uma quantidade indefinida, mas pelo processo ser simples seria 
	necessário um valor mais alto para de fato justificar o uso de threads.
